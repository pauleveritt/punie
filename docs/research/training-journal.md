# Training Experiment Journal

This document tracks all training experiments, evaluations, and decisions for Punie's local model fine-tuning.

## Cumulative Results

| Model/Adapter | Overall | Code Gen | Reasoning | Tool Calling | Status | Date |
|---------------|---------|----------|-----------|--------------|--------|------|
| **1.5B Base (Qwen2.5)** | **71.4%** | **100%** | **100%** | **33.3%** | ‚úÖ BEST | 2026-02-11 |
| successful-demo (1.5B) | 67.9% | 87.5% | 100% | 33.3% | ‚ö†Ô∏è Slight regression | 2026-02-11 |
| 30B Base (Qwen3) | 60.7% | 87.5% | 50% | 50% | ‚ö†Ô∏è Worse overall | 2026-02-11 |
| qwen-tool-format (1.5B) | 57.1% | 100% | 75% | 16.7% | ‚ùå Format didn't help | 2026-02-11 |
| glaive-function-calling (1.5B) | 53.6% | 70.8% | 66.7% | 33.3% | ‚ùå Significant regression | 2026-02-11 |
| baseline-diverse-python (1.5B) | 51.2% | 29.2% | 100% | 33.3% | ‚ùå Major regression | 2026-02-11 |
| tool-calling-synthetic (1.5B) | 32.1% | 62.5% | 0% | 33.3% | ‚ùå Catastrophic | 2026-02-11 |

**Note:** Previous baseline (41.7%) was from invalid server. All measurements before 2026-02-11 are unreliable.

## Experiments

### Phase 16: Baseline Evaluation & Training Failure Analysis (2026-02-11)

**Goal:** Establish valid baseline measurements after fixing mlx_lm.server, then evaluate all trained adapters.

**Context:** First valid measurements ever. Previous 17 HTML reports were invalid because server never loaded models (wrong command format `python -m mlx_lm.server` vs correct `python -m mlx_lm server`). Fixed in commit f4d3b3a.

**Commands run:**
```bash
# Add punie eval CLI command (Step 1-3 from plan)
# - Fix tool-call extraction in eval_runner.py
# - Add punie eval command to cli.py
# - Clean up 40 stale root files

# Step 4: Baseline evaluations
uv run punie eval --model mlx-community/Qwen2.5-Coder-1.5B-Instruct-4bit --port 8080 --output eval_base_model.html
uv run punie eval --adapter adapters/baseline-diverse-python-5k/ --port 8080 --output eval_baseline_diverse_python.html
uv run punie eval --adapter adapters/glaive-function-calling/ --port 8080 --output eval_glaive_function_calling.html
uv run punie eval --adapter adapters/tool-calling-synthetic/ --port 8080 --output eval_tool_calling_synthetic.html
uv run punie eval --adapter adapters/successful-demo/ --port 8080 --output eval_successful_demo.html
```

**Results:**
```
Base model:                 71.4% avg (100% code, 100% reason, 33.3% tools) ‚úÖ BEST
successful-demo:            67.9% avg (87.5% code, 100% reason, 33.3% tools)
glaive-function-calling:    53.6% avg (70.8% code, 66.7% reason, 33.3% tools)
baseline-diverse-python:    51.2% avg (29.2% code, 100% reason, 33.3% tools)
tool-calling-synthetic:     32.1% avg (62.5% code, 0% reason, 33.3% tools) ‚ùå WORST
```

**Critical Finding: ALL TRAINING HURTS PERFORMANCE**

Every single trained adapter performed worse than the untrained base model. This contradicts basic expectations of fine-tuning.

**Key Observations:**

1. **Tool Calling Fixed at 33.3% (1/3 prompts)**
   - No adapter improved tool calling
   - All stuck at same score
   - This was the primary target for training!
   - Suggests: data format mismatch or model size limitation

2. **Code Generation Severely Hurt**
   - Base: 100% perfect
   - baseline-diverse-python: 29.2% (destroyed!)
   - Worst regression of all categories

3. **Reasoning Sometimes Destroyed**
   - tool-calling-synthetic: 0% (complete failure)
   - Suggests catastrophic forgetting

4. **Hand-Authored Data Best**
   - successful-demo (8 examples): 67.9%
   - Close to base model despite smallest dataset
   - Quality >> quantity for small datasets

**Root Cause Hypotheses:**

### H1: Data Format Mismatch (HIGH LIKELIHOOD)

**Problem:** Training data has tool calls as plain text in assistant messages, but evaluation expects structured `tool_use` message parts.

**Evidence:**
- Tool calling never improves (stuck at 33.3%)
- Training on tool-calling datasets doesn't help
- Evaluation extracts tool names from `msg.parts` with `tool_name` attribute

**Training data format (mlx_lm):**
```json
{
  "messages": [
    {"role": "assistant", "content": "I'll use list_dir to find files..."}
  ]
}
```

**Evaluation expects (PydanticAI):**
```python
for part in msg.parts:
    if hasattr(part, "tool_name"):
        # Structured tool_use part, not text!
```

**Next Step:** Inspect training data to verify format.

### H2: Model Size Limitation (MEDIUM LIKELIHOOD)

**Problem:** 1.5B parameters may be too small for tool-calling LoRA.

**Evidence:**
- Base model already struggles (33.3%)
- LoRA adds only ~1-5M parameters (0.07-0.3%)
- Tool calling requires schema understanding + formatting

**Next Step:** Run baseline eval on 30B model for comparison.

### H3: Catastrophic Forgetting (MEDIUM LIKELIHOOD)

**Problem:** Training on narrow datasets causes forgetting of general capabilities.

**Evidence:**
- baseline-diverse-python: code gen dropped 70.8%
- tool-calling-synthetic: reasoning dropped to 0%
- successful-demo (smallest) performed best (less forgetting?)

**Potential Causes:**
- Too many iterations (100 may be excessive for 8-5000 examples)
- Learning rate too high (1e-5 may be aggressive)
- No regularization (dropout=0.0)

**Next Step:** Hyperparameter sweep with validation loss monitoring.

### H4: LoRA Configuration Issues (LOW-MEDIUM LIKELIHOOD)

**Current Config:**
- rank: 8 (default)
- alpha: 16 (default)
- dropout: 0.0 (no regularization!)
- Target modules: q_proj, v_proj (likely)

**Issues:**
- Rank 8 may be too low for structured output
- Missing o_proj, gate_proj targets?
- No dropout = overfitting on small datasets

**Next Step:** Try rank=16, alpha=32, dropout=0.1, all linear layers.

### H5: Evaluation-Training Distribution Mismatch (HIGH LIKELIHOOD)

**Problem:** Evaluation prompts may be out-of-distribution vs training data.

**Evaluation Examples:**
- "List all Python files in the workspace"
- "What is 15 * 23?"
- "Write a function to calculate factorial"

**Training Data Sources:**
- diverse-python-5k: General Python Q&A (likely Stack Overflow)
- glaive-function-calling: Function calling (format unknown)
- tool-calling-synthetic: Our generated examples

**If training data doesn't include file system operations or factorial examples, model won't generalize.**

**Next Step:** Inspect training data to verify coverage of evaluation scenarios.

**Decision: Cancel Step 5 (Progressive Pruning)**

Step 5 assumes training helps and measures the effect of dataset pruning. Since **all training hurts**, pruning won't fix the fundamental issue.

**Instead: Investigate root causes**

**Immediate Actions:**
1. ‚úÖ Document findings (this entry)
2. ‚¨ú Inspect training data format (diverse-python-5k, glaive)
3. ‚¨ú Check which tool-calling prompt succeeds (inspect HTML reports)
4. ‚¨ú Run 30B model baseline for size comparison

**Short-term:**
1. ‚¨ú Fix training data format if mismatch found
2. ‚¨ú Create properly formatted tool-calling dataset
3. ‚¨ú Retrain and evaluate with corrected data
4. ‚¨ú If still fails, switch to 30B model

**Files generated:**
- eval_base_model.html (71.4%)
- eval_baseline_diverse_python.html (51.2%)
- eval_glaive_function_calling.html (53.6%)
- eval_tool_calling_synthetic.html (32.1%)
- eval_successful_demo.html (67.9%)
- docs/research/training-journal.md (this updated journal)

**Infrastructure improvements:**
- Added `punie eval` CLI command
- Fixed tool-call extraction in eval_runner.py
- Cleaned up 40 stale root files (18 scripts, 17 HTML reports, 5 markdown files)
- Updated .gitignore for training artifacts

**Commits:**
- 35b1460: Add punie eval CLI and fix tool-call extraction
- f357f85: Document baseline evaluation and training failure analysis

---

### Phase 16.5: Overnight Investigation - Root Cause Found (2026-02-11)

**Goal:** Fix training data format, then test 30B model if that fails.

**What happened:**
After identifying training data format mismatch as the suspected root cause, attempted two solutions:

**Attempt 1: Qwen-Specific Training Format**
Created training data with Qwen's `<tool_call>` XML format:
```xml
<tool_call>
{"name": "read_file", "arguments": {"path": "/etc/hosts"}}
</tool_call>
```

Trained adapter with 10 examples, 50 iterations.

**Result:** Tool calling got WORSE (16.7% vs base 33.3%)
- Overall: 57.1% (down from 71.4%)
- Code: 100% (maintained!)
- Reasoning: 75% (down from 100%)
- Tools: 16.7% (worse than base)

**Conclusion:** Training data format is NOT the issue.

**Attempt 2: Test 30B Model**
Evaluated `Qwen3-Coder-30B-A3B-Instruct-4bit` base model to test if size is limiting factor.

**Result:** 30B is WORSE overall (60.7% vs 71.4%)
- Code: 87.5% (down from 100%!)
- Reasoning: 50% (down from 100%!)
- Tools: 50% (better, but still poor)

**Conclusion:** Model size is NOT the solution. Smaller 1.5B model is actually better.

**Root Cause Identified:**

**Architectural incompatibility between mlx_lm.server and PydanticAI:**

1. **mlx_lm.server:** Returns raw text from model (JSON in markdown code blocks)
2. **PydanticAI:** Expects OpenAI API's structured `tool_calls` objects
3. **The gap:** mlx_lm.server doesn't parse tool calls from model output

**Why training failed:**
- Training teaches text format (JSON, XML, etc.)
- Evaluation expects structured message parts
- No amount of training can bridge this architectural gap

**Why models regress:**
- Catastrophic forgetting on narrow datasets
- Overfitting on small examples (8-5000)
- Training optimizes wrong target (text vs structure)

**Final Recommendation:**

‚úÖ **Use 1.5B base model (no adapter) for production:**
- Best overall: 71.4%
- Perfect code generation: 100%
- Perfect reasoning: 100%
- Tool calling doesn't work due to architecture (not training)

‚ùå **Don't train adapters:**
- All training makes performance worse
- Base model is already optimal

‚¨ú **For tool calling to work, need to:**
- Modify mlx_lm.server to parse tool calls from text
- Or switch to llama.cpp/vLLM with native function calling
- Or use cloud APIs (OpenAI, Anthropic) for tool calling
- Or modify PydanticAI evaluation to parse JSON from text

**Files generated:**
- `data/qwen-tool-calling/` - 10 examples with XML format
- `adapters/qwen-tool-calling/` - Trained adapter (didn't help)
- `eval_qwen_tool_calling.html` - 57.1% (worse than base)
- `eval_30b_base_model.html` - 60.7% (worse than 1.5B)
- `create_qwen_tool_training_data.py` - Script for generating Qwen data
- `docs/research/tool-calling-investigation.md` - Complete analysis (15 pages)

**Key lessons:**
1. Infrastructure works perfectly
2. Base model is best (training hurts)
3. Architecture matters more than training data
4. Smaller can be better (1.5B > 30B overall)
5. Tool calling requires proper API support

---

### Phase 15: Progressive Dataset Pruning (2026-02-11)

**Goal:** Build infrastructure for progressive dataset filtering and comparison.

**What we did:**
- Added `punie dataset filter` command with language, Python version, and quality filters
- Added `punie dataset merge` command for combining datasets
- Created `eval_comparison.py` for side-by-side report comparison
- Built `test_progressive_pruning.py` end-to-end demonstration script

**Commands run:**
```bash
# Test progressive pruning workflow
uv run python test_progressive_pruning.py

# Results:
# - Created 9 test examples (7 train, 1 valid, 1 test)
# - Step A (Language): 86% retention (removed 1 non-English)
# - Step B (Python 3): 71% retention (removed 1 Python 2)
# - Step C (Quality): 71% retention (all passed quality check)

# Test CLI workflow with larger dataset
uv run punie dataset download sample --max 50 --output data/workflow-test/
uv run punie dataset validate data/workflow-test/
uv run punie dataset filter data/workflow-test/ --min-messages 3 --output data/workflow-filtered/
uv run punie train data/workflow-filtered/ --model mlx-community/Qwen2.5-Coder-1.5B-Instruct-4bit --output adapters/workflow-test --iters 10 --batch-size 2

# Training succeeded! Created 20MB adapter

# Test merge command
uv run punie dataset download sample --max 20 --output data/merge-test-a/
uv run punie dataset download sample --max 15 --output data/merge-test-b/
uv run punie dataset merge data/merge-test-a/ data/merge-test-b/ --output data/merged-dataset/ --name combined

# Merge succeeded: 20 + 15 = 35 examples (28 train, 3 valid, 4 test)

# All tests pass
uv run pytest tests/test_training_*.py -v
# 130 passed in 0.14s
```

**Results:**
- ‚úÖ All CLI commands functional and tested
- ‚úÖ Filtering infrastructure works perfectly
- ‚úÖ Training succeeds with 40+ examples (minimum dataset size identified)
- ‚úÖ Merging combines datasets correctly
- ‚úÖ 130 training tests passing (+4 new comparison tests)
- ‚úÖ All code passes ruff and ty checks

**Key findings:**
- **Minimum dataset size for training: 40+ examples**
- Filter retention rates clearly visible in CLI output
- Language filter detects non-English content
- Python version filter detects `print` statements, `has_key()`, `xrange`, etc.
- Quality filter checks message count and content length

**Files created:**
- `src/punie/training/eval_comparison.py` - Comparison report generation
- `tests/test_training_eval_comparison.py` - Comparison tests
- `test_progressive_pruning.py` - End-to-end demo script
- `docs/research/progressive-pruning-demo-results.md` - Demo documentation
- Updated `src/punie/cli.py` with filter and merge commands

**Artifacts generated:**
- `adapters/workflow-test/` - 20MB LoRA adapter (10 iterations, 40 examples)
- `data/workflow-test/` - Sample dataset (50 examples)
- `data/merged-dataset/` - Merged dataset (35 examples)
- `eval_pruning_baseline.html` - Baseline evaluation report

**Next steps:**
- Phase 15.2: Download real datasets (Dolma Wiki, RedPajama, KodCode)
- Phase 15.4: Hyperparameter tuning (grid search)
- Phase 15.5: Inference parameter tuning
- Phase 16: Tool calling data

---

### Phase 14: Training Data Infrastructure (2026-02-11)

**Goal:** Build framework for managing, validating, filtering, and writing training datasets.

**What we did:**
- Created dataset dataclasses (`ChatMessage`, `TrainingExample`, `TrainingDataset`, `DatasetStats`)
- Implemented validation functions (message count, roles, content checks)
- Built filtering functions (language, Python version, content quality)
- Created JSONL I/O (read/write datasets compatible with mlx_lm.lora)
- Implemented LoRA training runner with correct command format
- Added CLI commands: `train`, `dataset validate`, `dataset stats`, `dataset download`

**Commands run:**
```bash
# Create full pipeline test
uv run python test_full_training_pipeline.py

# Results:
# ‚úÖ Generated 50 examples (40 train, 5 valid, 5 test)
# ‚úÖ Validation passed
# ‚úÖ Baseline evaluation: 50.0% score
# ‚úÖ Training completed (10 iterations)
# ‚úÖ Adapted evaluation: 0.0% score (expected - too few iterations)
# ‚úÖ Comparison report generated

# All tests pass
uv run pytest tests/test_training_*.py -v
# 130 passed
```

**Critical fix discovered:**
- Training command format changed from `mlx_lm.lora` to `python -m mlx_lm lora --train`
- Parameter changed from `--lora-layers` to `--num-layers`
- Discovered during pipeline testing, fixed immediately

**Results:**
- ‚úÖ 130 training tests passing
- ‚úÖ Full end-to-end pipeline validated
- ‚úÖ Dataset I/O works with mlx_lm.lora format
- ‚úÖ All filters return tuples (kept, removed) for traceability
- ‚úÖ All code passes ruff and ty checks

**Files created:**
- `src/punie/training/dataset.py` - Core dataclasses
- `src/punie/training/dataset_validation.py` - Validation functions
- `src/punie/training/dataset_filters.py` - Filtering functions
- `src/punie/training/dataset_io.py` - JSONL I/O
- `src/punie/training/lora_config.py` - LoRA configuration
- `src/punie/training/train_runner.py` - Training runner
- `src/punie/training/downloaders.py` - Dataset downloaders
- `test_full_training_pipeline.py` - End-to-end test
- 8 test files with comprehensive coverage

**Next steps:**
- Phase 15: Progressive dataset pruning
- Download real datasets and apply filters step-by-step

---

### Phase 13: Evaluation Harness (2026-02-11)

### Phase 13: Evaluation Harness (2026-02-11)

**Goal:** Build standardized evaluation infrastructure to measure model performance.

**What we did:**
- Created `EvalPrompt` and `EvalSuite` dataclasses for organizing evaluation prompts
- Implemented scoring functions (`score_tool_calling`, `score_keyword_presence`, `score_prompt`)
- Built `EvalResult` and `EvalReport` dataclasses for storing and analyzing results
- Created `run_evaluation()` async function for full evaluation loop
- Added HTML report generation following `perf/report.py` pattern
- Created baseline evaluation suite with tool_calling, code_generation, and reasoning prompts

**Commands run:**
```bash
uv run pytest tests/test_training_eval_*.py -v
# All 46 tests passed
```

**Results:**
- ‚úÖ 368 total tests (up from 322, +46 for Phase 13)
- ‚úÖ Evaluation infrastructure ready for use
- ‚úÖ HTML reports for visualizing results
- ‚úÖ All code passes ruff and ty checks

**Files created:**
- `src/punie/training/eval_prompts.py` - Prompt and suite dataclasses
- `src/punie/training/eval_suites.py` - Pre-defined suite factory
- `src/punie/training/eval_results.py` - Result dataclasses and reporting
- `src/punie/training/eval_scoring.py` - Pure scoring functions
- `src/punie/training/eval_runner.py` - Evaluation orchestration
- `src/punie/training/eval_report.py` - HTML report generation
- 6 test files with 46 tests total

**Next steps:**
- Phase 13.5: Add CLI commands (`punie eval`, etc.)
- Phase 13.6: Create spec documentation
- Then move to Phase 14: Training Data Infrastructure

---

### Phase 12: Server Management Infrastructure (2026-02-11)

**Goal:** Build infrastructure for automated mlx_lm.server lifecycle management.

**What we did:**
- Created `ServerConfig` dataclass for server configuration
- Implemented `ServerProcess` for subprocess lifecycle management
- Added `create_server_model()` factory integration
- Built training speed benchmark utilities
- Created comprehensive test suite (20 tests)

**Commands run:**
```bash
uv run pytest tests/test_training_*.py -v
# All 20 tests passed

uv run ruff check src/punie/training/
# No issues

uv run ty check src/punie/training/
# All type checks passed
```

**Results:**
- ‚úÖ All 320 tests pass (up from 297)
- ‚úÖ 81%+ test coverage maintained
- ‚úÖ All code passes ruff and ty checks
- ‚úÖ Infrastructure ready for Phase 13 (Evaluation Harness)

**Files created:**
- `src/punie/training/__init__.py`
- `src/punie/training/server_config.py`
- `src/punie/training/server.py`
- `src/punie/training/benchmark.py`
- `tests/test_training_server_config.py`
- `tests/test_training_server.py`
- `tests/test_training_benchmark.py`
- `agent-os/specs/2026-02-11-server-management/` (4 files)

**Next steps:**
- Phase 12.4: Run actual benchmark with mlx-lm installed to verify 30B model is trainable
- Phase 13: Build evaluation harness using this infrastructure

---

### Validation: Real Model Evaluation (2026-02-11)

**Goal:** Validate evaluation infrastructure with real model.

**Model:** mlx-community/Qwen2.5-Coder-1.5B-Instruct-4bit (~1GB, small for testing)

**Commands run:**
```bash
uv add --dev mlx-lm  # Installed version 0.30.6
uv run python test_eval_only.py
```

**Results:**
```
‚úÖ Server management: WORKS
   - Server started automatically on port 8080
   - Server stopped cleanly after evaluation

‚úÖ Evaluation: WORKS
   - Overall Score: 41.7%
   - Success Rate: 100.0%
   - 2 prompts executed successfully

üìä Category Scores:
   - Reasoning: 50.0% (answered "2+2 = 4")
   - Code Generation: 33.3% (tried to use tools, got 1/3 keywords)

üìù Observations:
   - Base model scores are low (expected - no fine-tuning yet!)
   - Model attempted tool calling but doesn't know our tool names
   - HTML report generated successfully
   - Infrastructure works end-to-end
```

**Files generated:**
- `eval_quick_test.html` - Visual report with results

**Conclusion:**
‚úÖ Evaluation harness validated - ready for use!
‚úÖ Server management works automatically
‚úÖ Scoring functions work correctly
‚úÖ HTML reports generate properly

**Next:** Try training speed benchmark, then proceed to Phase 14.

---

### Benchmark: Training Speed (Pending)

**Goal:** Verify that LoRA training on Qwen3-Coder-30B-A3B-Instruct-4bit is feasible on M1 32GB.

**Model:** mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit (~15GB, 3B active parameters)

**Command to run:**
```bash
# First, create dummy dataset
uv run python -c "
from pathlib import Path
from punie.training.benchmark import create_dummy_dataset
create_dummy_dataset(Path('data/benchmark'), num_examples=5)
"

# Then run benchmark
uv run python -c "
import asyncio
from punie.training.benchmark import run_training_benchmark

async def main():
    result = await run_training_benchmark(
        model_path='mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit',
        num_iters=10,
    )
    print(f'Model: {result.model_path}')
    print(f'Seconds per iteration: {result.seconds_per_iter:.2f}')
    print(f'Total seconds: {result.total_seconds:.2f}')
    print(f'Iterations: {result.num_iters}')
    if result.peak_memory_gb:
        print(f'Peak memory: {result.peak_memory_gb:.2f} GB')

asyncio.run(main())
"
```

**Decision criteria:**
- ~1-5 sec/iter: ‚úÖ Proceed with 30B (100 iters = 2-8 min, fast iteration)
- ~10-30 sec/iter: ‚úÖ Still usable (100 iters = 15-50 min, acceptable)
- >60 sec/iter: ‚ùå Pivot to 7B model

**Results:** *(to be filled in after running benchmark)*

---

## Notes

- All experiments should be run from project root
- Update cumulative table after each evaluation
- Include raw command output in experiment entries
- Document any failures or unexpected behavior
- Track dataset versions and adapter paths
