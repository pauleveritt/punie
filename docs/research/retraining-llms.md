# Ethical & Viability Analysis: Toucan, KodCode, Dolma, RedPajama

Let me do a deep audit of each.

---

## Toucan

```
┌─────────────────────────────────────────────────────────┐
│ Dataset:  Agent-Eval/Toucan                             │
│ Size:     ~1.5M tool-agentic trajectories               │
│ License:  Apache 2.0                                    │
│ Source:   Salesforce Research (2024)                     │
│                                                         │
│ What it is:                                             │
│   Multi-turn agent conversations with tool calls        │
│   Specifically designed for training tool-use agents    │
│   Covers diverse tool calling scenarios                 │
│                                                         │
│ How it was made:                                        │
│   ⚠️  Synthetic generation pipeline                     │
│   ⚠️  Uses LLMs to generate trajectories               │
│   ✅  Published methodology in research paper            │
│   ✅  Salesforce Research — reputable institution        │
│   ✅  Designed specifically for open source training     │
│                                                         │
│ Ethical assessment:                                     │
│   ⚠️  Need to check WHICH model generated the data     │
│   ⚠️  If GPT-4 generated → same ToS concerns           │
│   ⚠️  If generated by open models → fully clean        │
│   ✅  Apache 2.0 license                                │
│   ✅  Academic paper with methodology                   │
│   ✅  Explicitly intended for training                   │
│                                                         │
│ Viability for your use case:                            │
│   ⭐⭐⭐⭐ EXCELLENT if ethically clear                   │
│   - Tool calling trajectories = exactly what you need   │
│   - Multi-turn = agent behavior training                │
│   - 1.5M examples = more than enough (subsample!)      │
│   - Would need filtering for Python-specific tasks      │
│                                                         │
│ Action items:                                           │
│   1. Read the paper to verify generation model          │
│   2. Check if they used open models (xLAM is their     │
│      own model family — possibly self-generated)        │
│   3. Filter for Python + English tasks                  │
│   4. Subsample — you need ~30-50K, not 1.5M            │
│                                                         │
│ Verdict: ✅ LIKELY VIABLE — verify generation method     │
└─────────────────────────────────────────────────────────┘
```

### Toucan Deep Dive

```python
from datasets import load_dataset

# Explore the dataset structure
toucan = load_dataset(
    "Agent-Eval/Toucan",
    split="train",
    streaming=True
)

# Check first few examples
for i, example in enumerate(toucan):
    if i >= 3:
        break
    print(f"Keys: {example.keys()}")
    print(f"Sample: {json.dumps(example, indent=2)[:500]}")
    print("---")


# Filter for Python-relevant tool calling
def is_python_agent_task(example: dict) -> bool:
    """Filter Toucan for Python coding agent tasks."""
    text = json.dumps(example).lower()

    # Must involve code/programming
    code_signals = [
        "python", "code", "function", "class",
        "script", "program", "implement", "debug",
        "run_python", "execute_code", "write_code"
    ]

    # Must be English
    # (Toucan is mostly English but verify)

    has_code = any(signal in text for signal in code_signals)

    # Check for tool calls in the conversation
    has_tools = "tool_call" in text or "function" in text

    return has_code and has_tools


# Estimate Python subset size
python_count = 0
total_count = 0
for example in toucan:
    total_count += 1
    if is_python_agent_task(example):
        python_count += 1
    if total_count >= 10000:  # Sample first 10K
        break

print(f"Python-relevant: {python_count}/{total_count}")
print(f"Estimated Python subset: ~{python_count / total_count * 1.5e6:.0f}")
```

---

## KodCode

```
┌─────────────────────────────────────────────────────────┐
│ Dataset:  KodCode                                       │
│ Size:     Varies by version                             │
│ License:  Check specific version                        │
│ Source:   Research project (2024)                        │
│                                                         │
│ What it is:                                             │
│   Synthetic code generation training data               │
│   Focused on coding tasks and solutions                 │
│   Multiple programming languages                        │
│                                                         │
│ How it was made:                                        │
│   ⚠️  Synthetically generated                           │
│   ⚠️  Generation pipeline uses LLMs                     │
│   ✅  Includes execution-verified code                   │
│   ✅  Code is tested / validated                         │
│                                                         │
│ Key strength:                                           │
│   Code examples are VERIFIED to actually run            │
│   This is unusual and very valuable                     │
│   Most synthetic code datasets have broken examples     │
│                                                         │
│ Ethical assessment:                                     │
│   ⚠️  Same synthetic generation concerns                │
│   ⚠️  Check which model was used for generation         │
│   ✅  Verification step adds genuine value              │
│   ✅  The verification is computational, not LLM-based  │
│                                                         │
│ Viability for your use case:                            │
│   ⭐⭐⭐ GOOD for the "modern Python" slice              │
│   - Verified code = higher quality training signal      │
│   - Would need Python-only filtering                    │
│   - Would need modernization check (Python version)     │
│   - NOT tool calling data — supplements it              │
│                                                         │
│ Verdict: ⚠️ VIABLE WITH CAVEATS — check generation model│
└─────────────────────────────────────────────────────────┘
```

---

## Dolma

```
┌─────────────────────────────────────────────────────────┐
│ Dataset:  allenai/dolma                                 │
│ Size:     ~3 trillion tokens                            │
│ License:  ODC-By 1.0 (Open Data Commons Attribution)    │
│ Source:   Allen Institute for AI (AI2)                   │
│                                                         │
│ What it is:                                             │
│   Massive pre-training corpus                           │
│   Used to train OLMo models                             │
│   Mix of web, code, academic, books, etc.               │
│                                                         │
│ Composition:                                            │
│   ├── Common Crawl (web text)          ~80%             │
│   ├── The Stack (code — deduplicated)  ~7%              │
│   ├── C4 (cleaned web text)            ~5%              │
│   ├── Reddit                           ~3%              │
│   ├── Semantic Scholar (papers)        ~3%              │
│   ├── Project Gutenberg (books)        ~1%              │
│   └── Wikipedia + Wikibooks            ~1%              │
│                                                         │
│ How it was made:                                        │
│   ✅  NO synthetic/LLM-generated content                 │
│   ✅  All sources are publicly available data            │
│   ✅  Extensive documentation of sources                 │
│   ✅  Built by non-profit research institute             │
│   ✅  Designed explicitly for open science               │
│   ✅  PII filtering applied                              │
│   ✅  Deduplication applied                              │
│   ✅  Transparent data pipeline (fully reproducible)     │
│                                                         │
│ Ethical assessment:                                     │
│   ✅  Best-documented large dataset available            │
│   ✅  Non-profit institution with ethical mission        │
│   ✅  No LLM-generated content                           │
│   ✅  Open Data Commons license                          │
│   ⚠️  Web crawl data has inherent noise                 │
│   ⚠️  Code portion comes from The Stack (same source    │
│      we discussed — permissive licenses)                │
│   ⚠️  Some web content may have copyright concerns      │
│      (same as any Common Crawl dataset)                 │
│                                                         │
│ Viability for your use case:                            │
│   ⭐⭐⭐ GOOD but needs significant filtering            │
│   - Massive — you need a tiny fraction                  │
│   - Code portion is The Stack (already discussed)       │
│   - English text portion great for language quality      │
│   - NOT tool calling data                               │
│   - Best for: continued pre-training English text       │
│                                                         │
│ Verdict: ✅ ETHICALLY EXCELLENT — best provenance of any │
│          large dataset                                   │
└─────────────────────────────────────────────────────────┘
```

### Extracting What You Need from Dolma

```python
"""
Dolma is HUGE. You only need tiny slices.
"""

from datasets import load_dataset

# Dolma code subset (derived from The Stack)
dolma_code = load_dataset(
    "allenai/dolma",
    name="code",  # Just the code portion
    split="train",
    streaming=True  # MUST stream — too large to download
)

# Dolma English text (for language quality)
dolma_wiki = load_dataset(
    "allenai/dolma",
    name="wiki",  # Wikipedia portion
    split="train",
    streaming=True
)

# What to extract for your use case:
#
# From code portion:
#   - Same filtering as Stack v2 (modern Python, etc.)
#   - ~5-10K high quality examples
#
# From wiki/web portion:
#   - Technical English writing about programming
#   - Python documentation style text
#   - ~5-10K examples for English quality
#
# Total from Dolma: ~10-20K examples
# Role: Supporting data, not primary
```

---

## RedPajama

```
┌─────────────────────────────────────────────────────────┐
│ Dataset:  togethercomputer/RedPajama-Data-V2            │
│ Size:     V1: ~1.2T tokens, V2: ~30T tokens            │
│ License:  Apache 2.0 (V1), varies by subset (V2)       │
│ Source:   Together AI                                    │
│                                                         │
│ What it is:                                             │
│   Large pre-training corpus                             │
│   V1: Reproduction of LLaMA training data               │
│   V2: Massive web crawl with quality signals            │
│                                                         │
│ Composition (V1):                                       │
│   ├── Common Crawl                     ~78%             │
│   ├── C4                               ~15%             │
│   ├── GitHub (code)                    ~5%              │
│   ├── Wikipedia                        ~2%              │
│   ├── Books                            ~0.5%            │
│   ├── ArXiv                            ~2%              │
│   └── StackExchange                    ~2%              │
│                                                         │
│ How it was made:                                        │
│   ✅  NO synthetic/LLM-generated content                 │
│   ✅  Transparent reproduction methodology               │
│   ✅  Quality scoring per document                       │
│   ✅  Language identification included                   │
│   ⚠️  V2 quality signals help but still noisy           │
│                                                         │
│ Ethical assessment:                                     │
│   ✅  Apache 2.0 license (V1)                           │
│   ✅  No LLM contamination                               │
│   ✅  Transparent methodology                            │
│   ✅  Together AI is reputable                           │
│   ⚠️  GitHub portion may include various licenses       │
│   ⚠️  Web crawl copyright concerns (standard caveat)   │
│                                                         │
│ Viability for your use case:                            │
│   ⭐⭐ MODERATE — similar role to Dolma                  │
│   - Code portion overlaps with Stack/Dolma              │
│   - StackExchange portion very valuable!                │
│   - English text portion useful                         │
│   - NOT tool calling data                               │
│                                                         │
│ Key advantage over Dolma:                               │
│   StackExchange data! Q&A about Python programming      │
│   is excellent training data for a coding assistant     │
│                                                         │
│ Verdict: ✅ ETHICALLY SOUND — good supplementary source  │
└─────────────────────────────────────────────────────────┘
```

### RedPajama StackExchange: Hidden Gem

```python
"""
The StackExchange portion of RedPajama is particularly
valuable for a coding agent — real Q&A about Python!
"""


# StackExchange data includes:
# - StackOverflow Python questions and answers
# - Code Review StackExchange
# - Software Engineering StackExchange
#
# This is REAL human-written Q&A about coding
# Not synthetic, not LLM-generated
# CC-BY-SA licensed (StackExchange's license)

# Extract Python Q&A from StackExchange portion
def extract_python_qa(example: dict) -> bool:
    """Filter for Python-related StackExchange posts."""
    text = example.get("text", "").lower()
    tags = example.get("meta", {}).get("tags", "").lower()

    is_python = (
            "python" in tags or
            "python-3" in tags or
            "pydantic" in tags or
            "pytest" in tags
    )

    # Also check content for Python code blocks
    has_python_code = (
            "def " in text and
            "import " in text
    )

    return is_python or has_python_code


# Convert to instruction format for training
def stackoverflow_to_training(qa: dict) -> dict:
    """Convert a StackOverflow Q&A to instruction format."""
    return {
        "messages": [
            {
                "role": "system",
                "content": (
                    "You are a Python 3.12+ expert. "
                    "Provide clear, well-typed, modern Python solutions."
                )
            },
            {
                "role": "user",
                "content": qa["question"]
            },
            {
                "role": "assistant",
                "content": modernize_answer(qa["accepted_answer"])
            }
        ]
    }
```

---

## Comparison Matrix

```
┌───────────────┬──────────┬──────────┬───────────┬────────────┬──────────────┐
│ Dataset       │ Ethical? │ Tool     │ Code      │ English    │ Your Use     │
│               │          │ Calling? │ Quality?  │ Text?      │ Case Fit     │
├───────────────┼──────────┼──────────┼───────────┼────────────┼──────────────┤
│ Toucan        │ ✅/⚠️    │ ⭐⭐⭐⭐   │ ⭐⭐       │ ⭐⭐        │ ⭐⭐⭐⭐ Tool  │
│               │ Verify!  │ Primary  │           │            │ calling core │
│               │          │ purpose  │           │            │              │
├───────────────┼──────────┼──────────┼───────────┼────────────┼──────────────┤
│ KodCode       │ ⚠️       │ ❌       │ ⭐⭐⭐⭐    │ ⭐⭐        │ ⭐⭐⭐ Code   │
│               │ Verify!  │          │ Verified! │            │ generation   │
│               │          │          │           │            │ supplement   │
├───────────────┼──────────┼──────────┼───────────┼────────────┼──────────────┤
│ Dolma         │ ✅✅✅    │ ❌       │ ⭐⭐       │ ⭐⭐⭐⭐     │ ⭐⭐ English  │
│               │ Gold     │          │ Has Stack │ Best       │ text quality │
│               │ standard │          │ subset    │ documented │              │
├───────────────┼──────────┼──────────┼───────────┼────────────┼──────────────┤
│ RedPajama     │ ✅✅     │ ❌       │ ⭐⭐       │ ⭐⭐⭐      │ ⭐⭐⭐ Stack  │
│               │ Good     │          │ GitHub    │ Good       │ Exchange Q&A │
│               │          │          │ subset    │            │ is great     │
├───────────────┼──────────┼──────────┼───────────┼────────────┼──────────────┤
│ Stack v2      │ ✅✅     │ ❌       │ ⭐⭐⭐⭐    │ N/A        │ ⭐⭐⭐ Code   │
│               │ Good     │          │ Best code │ (code only)│ examples     │
│               │          │          │ source    │            │              │
├───────────────┼──────────┼──────────┼───────────┼────────────┼──────────────┤
│ Self-gen      │ ✅✅✅    │ ⭐⭐⭐⭐   │ ⭐⭐⭐     │ ⭐⭐⭐      │ ⭐⭐⭐⭐ Most │
│ (Qwen/Llama)  │ Fully    │ Custom   │ Modern    │ Custom     │ targeted     │
│               │ clean    │ to YOUR  │           │            │              │
│               │          │ tools    │           │            │              │
└───────────────┴──────────┴──────────┴───────────┴────────────┴──────────────┘
```

---

## Revised Recommended Pipeline

```
┌──────────────────────────────────────────────────────────┐
│          FINAL ETHICAL DATA PIPELINE                     │
│                                                          │
│  TOOL CALLING (35-40% of training data):                 │
│  ├── Toucan (filtered for Python)        ~15-20K         │
│  │   └── IF generation model is verified ethical         │
│  ├── Self-generated via Qwen2.5-72B     ~15-20K         │
│  │   └── Custom to YOUR Pydantic AI tools                │
│  ├── Hand-crafted examples               ~500-1K         │
│  │   └── Gold standard for your specific agent           │
│  └── Mined from OSS (Pydantic AI, etc.) ~1-2K           │
│                                                          │
│  MODERN PYTHON CODE (25-30%):                            │
│  ├── KodCode (Python, verified subset)   ~10-15K         │
│  │   └── IF generation model is verified ethical         │
│  ├── Stack v2 (ultra-curated modern)     ~5-10K          │
│  │   └── Only 3.10+ with types and docstrings            │
│  └── Python 3.12 synthetic features      ~2-3K           │
│      └── Self-generated with Qwen2.5-72B                 │
│                                                          │
│  ENGLISH Q&A + INSTRUCTION (15-20%):                     │
│  ├── RedPajama StackExchange (Python)    ~8-10K          │
│  │   └── Real human Q&A, CC-BY-SA                        │
│  └── Dolma wiki (technical English)      ~3-5K           │
│      └── ODC-By, high quality text                       │
│                                                          │
│  STRUCTURED OUTPUT (10-15%):                             │
│  ├── Self-generated Pydantic examples    ~5-8K           │
│  └── Mined from Instructor/Pydantic repos ~1-2K          │
│                                                          │
│  AGENT TRACES / ERROR RECOVERY (5-10%):                  │
│  ├── Toucan multi-turn traces            ~3-5K           │
│  └── Self-generated error scenarios      ~2-3K           │
│                                                          │
│  ─────────────────────────────────────────────           │
│  TOTAL: ~55-100K examples                                │
│  ALL ethically sourced or verified ✅                     │
│  Trainable on M1 32GB ✅                                  │
│  Cost: ~$100-200 for generation + your time              │
└──────────────────────────────────────────────────────────┘
```

---

## Verification Script for Toucan & KodCode

```python
"""
Before using Toucan or KodCode, verify their generation source.
"""


def audit_dataset_provenance(dataset_name: str):
    """Check a dataset's ethical provenance."""

    checks = {
        "has_paper": False,
        "generation_model_disclosed": False,
        "generation_model_permissive": False,
        "license_clear": False,
        "pii_handled": False,
        "methodology_documented": False,
    }

    print(f"\n{'=' * 60}")
    print(f"AUDITING: {dataset_name}")
    print(f"{'=' * 60}")

    # 1. Check dataset card on HuggingFace
    print("\n1. Check the dataset card on HuggingFace:")
    print(f"   https://huggingface.co/datasets/{dataset_name}")
    print("   Look for:")
    print("   - 'Generated by' or 'Created using' section")
    print("   - License field")
    print("   - Paper link")

    # 2. Check associated paper
    print("\n2. Find and read the associated paper:")
    print("   Look for methodology section describing:")
    print("   - Which model generated the data")
    print("   - Whether outputs are from a permissive model")
    print("   - Data validation/filtering steps")

    # 3. Specific things to look for
    print("\n3. RED FLAGS to watch for:")
    print("   ❌ 'Generated using GPT-4' or 'GPT-3.5'")
    print("   ❌ 'Using OpenAI API'")
    print("   ❌ 'Claude-generated'")
    print("   ❌ No mention of generation method at all")

    print("\n4. GREEN FLAGS:")
    print("   ✅ 'Generated using Llama/Qwen/Mistral'")
    print("   ✅ 'Generated using our own model (xLAM/etc.)'")
    print("   ✅ 'Human-annotated'")
    print("   ✅ 'Extracted from open source repositories'")
    print("   ✅ 'Derived from documentation'")

    # 4. For Toucan specifically
    if "toucan" in dataset_name.lower():
        print("\n5. TOUCAN-SPECIFIC checks:")
        print("   - Salesforce also created xLAM models")
        print("   - If Toucan was generated by xLAM → likely clean")
        print("   - xLAM itself may have been trained on GPT data")
        print("   - Check the full lineage: model → training data → generation")
        print("   - This is the 'data laundering' concern:")
        print("     GPT-4 → trains Model A → Model A generates data")
        print("     Is this ethical? Debatable but generally accepted")

    # 5. For KodCode specifically
    if "kodcode" in dataset_name.lower():
        print("\n5. KODCODE-SPECIFIC checks:")
        print("   - Key value: code is execution-verified")
        print("   - Verification step is computational, not LLM")
        print("   - The GENERATION step needs checking")
        print("   - The VERIFICATION step is ethically clean")

    return checks


audit_dataset_provenance("Agent-Eval/Toucan")
audit_dataset_provenance("kodcode")  # Check exact HF path
```

---

## The "Data Laundering" Question

```
An important ethical nuance you should be aware of:

The chain:
  GPT-4 generates data
    → Used to train Open Model A
      → Open Model A generates new data
        → Used to train your model

Is this ethical? The community is split:

View 1: "It's fine"
  - Model A is a new model with its own license
  - Its outputs are its own, not GPT-4's
  - This is how knowledge transfer works
  - The alternative is no open source AI

View 2: "It's still tainted"
  - The capability traces back to GPT-4
  - OpenAI's training investment is being extracted
  - It's ethically questionable even if legally gray

View 3: "It depends on the degree"
  - One generation removed: questionable
  - Two+ generations removed: probably fine
  - Mixed with real data: probably fine
  - The model adds its own capability through training

Practical recommendation:
  Be TRANSPARENT about your data sources.
  Document the provenance chain.
  Let users of your model make their own judgment.
  Prefer first-party sources where possible.
```

---

## Final Practical Recommendation

```
TIER 1 — Use with confidence:
  ✅ Dolma (AI2 — gold standard ethics)
  ✅ RedPajama StackExchange subset
  ✅ Stack v2 (permissive license filter)
  ✅ Self-generated with Qwen2.5/Llama 3.1
  ✅ Hand-crafted examples
  ✅ Mined from MIT/Apache licensed repos

TIER 2 — Use after verifying generation model:
  ⚠️ Toucan (verify generation chain)
  ⚠️ KodCode (verify generation chain)
  ⚠️ Gorilla BFCL (mostly clean, small synthetic component)

TIER 3 — Avoid for ethical build:
  ❌ Glaive (GPT-generated)
  ❌ Unverified synthetic datasets
  ❌ Datasets with no provenance documentation

If Toucan and KodCode check out after verification,
your dataset becomes significantly stronger with very
little additional effort. They're worth investigating.
```

Would you like me to help build the verification scripts to audit Toucan and KodCode, or move forward with designing the
self-generation pipeline using Qwen2.5-72B as the guaranteed-clean approach?