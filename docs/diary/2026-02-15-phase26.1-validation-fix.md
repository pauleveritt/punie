# Phase 26.1: Validation Script Fix (Feb 15, 2026)

## Summary

**Phase 26 was never broken!** The catastrophic 28% accuracy was caused by a validation script bug. After fixing the prompt format, Phase 26 achieved **88% accuracy** and met all success criteria.

## The Bug

**Root cause:** `scripts/test_phase26_validation.py` line 154 used wrong prompt format:

```python
# WRONG (line 154):
prompt = f"User: {query}\nAssistant:"

# CORRECT (after fix):
prompt = format_prompt(query)  # Uses Qwen3 ChatML template
```

**Impact:** Training used ChatML format (`<|im_start|>system\n...<|im_end|>`), but validation used plain text. This caused the model to receive completely out-of-distribution input.

## Results Comparison

### Before Fix (Broken Prompt)
- Overall: 28% (7/25) ‚ùå
- Field access rate: 15% (3/20) ‚ùå
- Hallucinations: JavaScript, football rules, empty responses

### After Fix (Correct ChatML)
- Overall: **88% (22/25)** ‚úÖ Target: ‚â•80%
- Field access rate: **85% (17/20)** ‚úÖ Target: ‚â•80%
- Discrimination: **100% (5/5)** ‚úÖ Target: 100%

## Fair Baseline: Phase 23 vs Phase 26

Tested both models on same 25-query suite with correct prompts:

| Metric | Phase 23 | Phase 26 | Improvement |
|--------|----------|----------|-------------|
| Overall accuracy | 24% (6/25) | **88% (22/25)** | **+64%** |
| Field access rate | 5% (1/20) | **85% (17/20)** | **+80%** |
| Discrimination | 100% (5/5) | 100% (5/5) | Maintained |

**Phase 26 is a massive success:**
- Field access training worked (+80% improvement)
- No regression on discrimination (100% maintained)
- +64% overall improvement from Phase 23

## Implementation Details

### Three Fixes Applied

**1. Added `format_prompt()` function** (line 66-74):
```python
def format_prompt(query: str) -> str:
    """Format query in Qwen3 ChatML template."""
    return (
        "<|im_start|>system\n"
        "You are Punie, an AI coding assistant that helps with Python development via PyCharm.<|im_end|>\n"
        "<|im_start|>user\n"
        f"{query}<|im_end|>\n"
        "<|im_start|>assistant\n"
    )
```

**2. Updated line 154** to use correct format:
```python
prompt = format_prompt(query)  # Was: f"User: {query}\nAssistant:"
```

**3. Dynamic output filenames** (lines 276-277):
```python
model_name = Path(model_path).name
output_file = Path(f"logs/phase26_validation_{model_name}.json")
```

### Validation Results

**Phase 26 Model** (`fused_model_qwen3_phase26_6bit/`):
- Overall: 88% (22/25) ‚úÖ
- Field access: 85% (17/20) ‚úÖ
- Category breakdown:
  - A. Discrimination: 100% (5/5) ‚úÖ
  - B. Conditional: 100% (5/5) ‚úÖ
  - C. Field access: 60% (3/5) ‚ö†Ô∏è
  - D. Iteration: 100% (5/5) ‚úÖ
  - E. Multi-step: 80% (4/5) ‚úÖ

**Phase 23 Model** (`fused_model_qwen3_phase23_ty_5bit/`):
- Overall: 24% (6/25)
- Field access: 5% (1/20)
- Only discrimination worked (100%)
- All field access categories failed (0% across B, C, D)

## Category C Analysis (60% - Below Target)

Two queries failed in Category C:
1. "How many type errors are in src?" ‚Üí Used `grep` instead of `typecheck()`
2. "How many ruff violations are fixable?" ‚Üí Used `ruff` CLI instead of `ruff_check()`

**Hypothesis:** Model needs more examples showing when to prefer typed tools over CLI commands.

**Impact:** Minor - field access rate overall is 85% (above target). These two queries still call tools, just not typed tools.

## Critical Learning

**Loss metrics are NOT sufficient for validation:**
- Training loss: 3.050 ‚Üí 0.616 (excellent 79.8% reduction)
- But broken validation showed 28% accuracy

**Train/test consistency is CRITICAL:**
- Prompt format mismatch caused 60-point accuracy drop (28% ‚Üí 88%)
- Always validate with exact training prompt format
- Even small format differences can cause catastrophic failure

**The model worked correctly all along** - only the validation script was broken!

## Production Recommendation

**‚úÖ DEPLOY Phase 26 as new production standard**

**Current Production Model:**
- `fused_model_qwen3_phase26_6bit/` (23 GB)
- 88% overall accuracy (+64% from Phase 23)
- 85% field access rate (+80% from Phase 23)
- 100% discrimination (maintained)

**Archived Model:**
- `fused_model_qwen3_phase23_ty_5bit/` (20 GB)
- Keep for rollback if issues discovered
- Superseded by Phase 26 on all metrics

## Next Steps

**Optional Future Work:**

**Phase 26.2: Improve Category C (60% ‚Üí 80%)**
- Generate 20 examples showing typed tool preference
- Train incrementally on Phase 26 adapters
- Target: overall 88% ‚Üí 92%

**Higher Priority: LSP Integration**
- More impactful than marginal Category C improvement
- Field access already at 85%, close to target
- Focus on language server capabilities instead

**Recommendation:** Focus on LSP integration rather than Category C optimization.

## Files Modified

- `scripts/test_phase26_validation.py` - Added ChatML format, fixed prompt
- `docs/diary/2026-02-15-phase26-field-access-training.md` - Updated with corrected results
- `/Users/pauleveritt/.claude/projects/-Users-pauleveritt-projects-pauleveritt-punie/memory/MEMORY.md` - Updated Phase 26 entry

## Files Created

- `logs/phase26_validation_fused_model_qwen3_phase26_6bit.json` - Phase 26 results (88%)
- `logs/phase26_validation_fused_model_qwen3_phase23_ty_5bit.json` - Phase 23 baseline (24%)
- `logs/phase26_validation_run.log` - Phase 26 validation log
- `logs/phase23_validation_run.log` - Phase 23 validation log
- `docs/diary/2026-02-15-phase26.1-validation-fix.md` - This document

## Success Criteria

- [x] Overall accuracy ‚â•80% ‚úÖ **Achieved 88%**
- [x] Field access rate ‚â•80% ‚úÖ **Achieved 85%**
- [x] Discrimination 100% ‚úÖ **Maintained**
- [x] No regression from Phase 23 ‚úÖ **+64% improvement**

## Conclusion

Phase 26.1 was a **bug fix, not a model retrain**. The original Phase 26 model works correctly when validated with proper prompt format. This demonstrates the critical importance of train/test consistency in LLM fine-tuning.

**Phase 26 is production-ready and deployed!** üéâ
