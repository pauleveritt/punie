---
date: 2026-02-13
summary: Tracker of model size, memory, speed, tool use, and accuracy across phases with key findings.
---

# Model Performance Tracker

*2026-02-13*


**Purpose:** Track model size, memory usage, and performance improvements across training phases.

**Last Updated:** February 13, 2026 - Phase 2 Discovery

---

## Quick Reference Table - All Phases

| System | Model Size | Memory (Inference) | Speed | Autonomous Tools? | Accuracy | Status |
|--------|------------|-------------------|-------|-------------------|----------|--------|
| **Claude Code** | 0 (cloud) | 0 GB local | **2.41s** ‚ö° | ‚úÖ Yes | 100% | Baseline |
| **30B Baseline** | ~60 GB | ~16 GB | **~27s** | ‚úÖ Yes | 100% | Too slow |
| **7B Phase 0** | 14 GB + 44MB adapter | ~6-8 GB | **21.06s** | ‚ùå No (memorized) | 100% | Broken |
| **7B Phase 1** | 14 GB + 44MB adapter | Unknown | **N/A** (infinite loop) | ‚úÖ **Yes!** (but loops) | N/A (no answer) | Progress! |

### Phase 0 Baseline (Original Training)

### Detailed Metrics

#### **Model Size**
- **30B:** ~60 GB (full model)
- **7B + LoRA:** 14.044 GB total (77% smaller than 30B)
  - 7B base: 14 GB (4-bit quantized)
  - LoRA adapter: 44 MB
- **Claude Code:** 0 GB local (cloud-based)

#### **Memory Usage**
- **30B:** ~16 GB RAM (‚ùå **crashed system**)
- **7B Distilled:** ~6-8 GB RAM (50-60% reduction vs 30B)
  - Training peak: 23.63 GB
  - Inference: ~6-8 GB
- **Claude Code:** 0 GB local

#### **Speed Performance** (Protocol search query)
- **Claude Code:** 2.41s (baseline) - **11.2x faster than 30B**
- **7B Distilled:** 21.06s - 1.3x faster than 30B, but 8.7x slower than Claude
- **30B:** ~27s

#### **Training Costs**
- **7B LoRA Training:**
  - Duration: 11 minutes (50 iterations)
  - Dataset: 69 examples
  - Loss reduction: 81% (1.269 ‚Üí 0.246)
  - Peak memory: 23.63 GB
  - Adapter size: 44 MB

### Key Findings

**Claude Code:**
- ‚úÖ Fastest (2.41s)
- ‚úÖ No local resources
- ‚úÖ Fully autonomous tool use
- ‚úÖ Works on any codebase
- ‚ùå Requires API key/internet

**7B Distilled Model:**
- ‚úÖ 77% smaller than 30B
- ‚úÖ 50-60% less memory than 30B
- ‚úÖ Accurate (100%)
- ‚ùå **Memorized patterns** instead of learning autonomous tool use
- ‚ùå Only works on familiar codebases
- ‚ùå Training data too small (69 examples from one codebase)

**30B Model:**
- ‚úÖ Autonomous tool use
- ‚úÖ Works on any codebase
- ‚ùå 11.2x slower than Claude Code
- ‚ùå 16GB RAM (system crash risk)
- ‚ùå Not production viable

### Critical Issue Discovered

**The 7B model memorized instead of learning:**
- Gave correct answers directly without calling tools
- Learned "this codebase has these protocols" not "how to search any codebase"
- Root cause: Insufficient training data (need 1,000-5,000 examples from 20+ codebases)

### Test Query Used
"Which classes in this codebase subclass from Protocol?"

**Expected behavior:** Search codebase with grep/find tools, analyze results, provide answer
**Actual 7B behavior:** Direct answer without tool calls (memorization)

---

## Phase 1: Training Data Pipeline Fix ‚úÖ COMPLETED (With Issues)

**Date:** February 13, 2026
**Goal:** Fix critical tool-call format mismatch and prepare for proper tool-calling training
**Status:** Training successful, but discovered new critical bug

### Changes Made

1. **Fixed tool-call format:**
   - Changed from `"tool"` key to `"name"` key (matches parser)
   - Updated 30 hand-authored examples
   - Regenerated 69 converted examples

2. **Added converter to training pipeline:**
   - `train_lora.sh` now runs converter before training
   - `run_full_distillation.sh` includes converter step
   - Ensures correct format reaches mlx_lm.lora

3. **Wired stop_sequences:**
   - Local agents default to QWEN_STOP_SEQUENCES
   - Added to config and factory
   - (Results: NOT working - still generating garbage tokens)

4. **Memory requirements:**
   - Had to use batch_size=1 (not 2) to avoid OOM
   - Actual peak: 18.902 GB
   - batch_size=2 crashed with "Insufficient Memory" error

### Actual Results

| Metric | Phase 0 | Phase 1 Actual | Change | Status |
|--------|---------|----------------|--------|--------|
| Training loss | 1.269 ‚Üí 0.246 | 1.077 ‚Üí 0.096 | Better (91% vs 81%) | ‚úÖ Improved |
| Training peak memory | 23.63 GB | 18.902 GB | -20% (not -40%) | ‚ö†Ô∏è Less than expected |
| Batch size | 4 | 1 (not 2!) | -75% | ‚ùå Slower training |
| Training iterations | 50 | 206 | 4x more | ‚ö†Ô∏è Took longer |
| **Autonomous tools?** | ‚ùå No (memorized) | ‚úÖ **YES!** | Fixed! | üéâ **BREAKTHROUGH** |
| Tool execution | Direct answer | 20+ tool calls | Works but loops | ‚ö†Ô∏è New bug |
| Stop sequences | N/A | Not working | No improvement | ‚ùå Failed |

### üéâ BREAKTHROUGH: Model Uses Tools!

**Phase 0 behavior (broken):**
```
User: "Which classes subclass from Protocol?"
Model: "Found 6 protocols: HttpAppFactory, Client, Agent..." [Direct answer - memorized]
```

**Phase 1 behavior (fixed but looping):**
```
User: "Which classes subclass from Protocol?"
Model: run_command(grep -r 'class.*Protocol' .)
Model: run_command(grep -r 'class.*Protocol' .)  [Repeats 20+ times]
Model: run_command({})  [Empty args]
Model: [Never gives final answer - stuck in loop]
```

### Critical Issues Discovered

1. **‚úÖ FIXED: Memorization ‚Üí Tool Usage**
   - Model now calls tools instead of memorizing
   - Uses appropriate commands (`find`, `grep`)
   - Correctly formatted tool calls with `"name"` key

2. **‚ùå NEW BUG: Infinite Tool Loop**
   - Model calls same tool 20+ times
   - Doesn't process tool results
   - Never stops to give final answer
   - Some calls have empty arguments `{}`

3. **‚ùå Stop Sequences Don't Work**
   - Still generating `<|im_end|>` garbage tokens
   - No speedup observed
   - Need to investigate why stop_sequences aren't being applied

4. **‚ùå Memory Higher Than Expected**
   - Needed batch_size=1 instead of 2
   - Peak 18.9 GB instead of target 12-14 GB
   - 7B model + training might need more optimization

### Root Cause Analysis

**Why infinite loop?**

The training data uses **placeholder tool results**:
```
Tool result: [Tool execution completed]
```

The model learned:
1. ‚úÖ When to call tools
2. ‚úÖ How to format tool calls
3. ‚ùå How to interpret results
4. ‚ùå When to stop and give answer

**Evidence:** Converter warning we ignored:
```
‚ö†Ô∏è  Note: Current data uses placeholder tool results.
   Run generate_training_data.py with updated capture logic for real results.
```

### Training Details

**Configuration:**
- Model: Qwen2.5-Coder-7B-Instruct-4bit
- Batch size: 1 (down from planned 2)
- Learning rate: 1e-4
- LoRA rank: 16
- Iterations: 206 (up from 50)
- Training time: ~20 minutes

**Loss Progression:**
- Train loss: 1.077 ‚Üí 0.096 (91% reduction)
- Val loss: 2.267 ‚Üí 0.436 (81% reduction)
- Test loss: 0.899, Test ppl: 2.458

**Memory:**
- Peak: 18.902 GB
- Training tokens: 117,940

### Next Steps (Phase 2)

**Fix the infinite loop:**
1. Generate training data with REAL tool results (not placeholders)
2. Update `generate_training_data.py` to capture actual outputs
3. Train model to recognize when tools have provided sufficient info
4. Add examples of "now I have enough info, here's the answer" pattern

**Fix stop sequences:**
5. Debug why QWEN_STOP_SEQUENCES aren't being applied
6. Verify they're passed to model inference correctly
7. Test with simple example to confirm they work

**Optimize memory:**
8. Investigate why batch_size=2 causes OOM
9. Try reducing sequence length or other parameters
10. Consider using 3-bit quantization

---

## Phase 2: Fix Infinite Loop with Real Tool Results ‚ùå FAILED (Critical Discovery)

**Date:** February 13, 2026
**Goal:** Train on examples with real tool results to fix infinite loop
**Status:** Failed - discovered fundamental format mismatch

### Changes Attempted

**Step 1: Hand-Authored Examples (Failed)**
1. Tried merging 30 hand-authored examples (with real tool results)
2. Used `{messages}` format instead of `{text}` format
3. Trained for 130 iterations with batch_size=1
4. Training succeeded (val loss: 3.095 ‚Üí 0.721)
5. **Result: Model completely broken**

### Critical Discovery: Format Mismatch

**The hand-authored examples use MARKDOWN-formatted tool calls:**
```json
{
  "role": "assistant",
  "content": "I'll use the run_command tool.\n\n```json\n{\"name\": \"run_command\", ...}\n```"
}
```

**This teaches the model to GENERATE markdown text, not USE the function calling API!**

**Phase 2 behavior (worse than Phase 1):**
```
User: "What is Python?"
Model: <hallucinates fake conversation>
  I'll use the run_command tool...
  <|im_start|>user
  Tool result: Running main...
  <|im_start|>assistant
  The file runs successfully...
  [Repeats infinitely - pure hallucination]
```

The model learned to:
- ‚ùå Generate fake tool calls as text
- ‚ùå Generate fake tool results as text
- ‚ùå Generate fake conversation structure
- ‚ùå Never actually call the function calling API
- ‚ùå Loop infinitely in this hallucinated conversation

### Root Cause

1. **Phase 1** used `{text: ...}` format with Qwen tokens ‚Üí model learned to CALL tools via API
2. **Phase 2** used `{messages}` format with markdown ‚Üí model learned to TALK ABOUT tools as text
3. The hand-authored examples were written for HUMAN demonstration, not model training
4. They teach markdown generation, not structured API usage

### Format Comparison

| Format | Phase | Tool Behavior | Result |
|--------|-------|---------------|--------|
| `{text}` + Qwen tokens | Phase 1 | ‚úÖ Calls tools via API | Loops but calls real tools |
| `{messages}` + markdown | Phase 2 | ‚ùå Generates markdown text | Hallucinates fake conversation |

### Recovery Plan

**Abandoned Approach:**
- ‚ùå Hand-authored examples with markdown tool calls
- ‚ùå `{messages}` format

**Correct Approach:**
- ‚úÖ Keep `{text}` format with Qwen tokens (worked in Phase 1)
- ‚úÖ Fix `generate_training_data.py` to capture real tool results
- ‚úÖ Use generated examples WITH real results (not placeholder)
- ‚úÖ Regenerate training data from 30B server with fixed generator

### Training Metrics (Before Failure Discovered)

**Configuration:**
- Model: Qwen2.5-Coder-7B-Instruct-4bit
- Batch size: 1
- Learning rate: 1e-4
- Iterations: 130
- Dataset: 40 train + 5 valid (45 total)

**Loss Progression:**
- Train loss: 1.283 ‚Üí 0.141 (89% reduction)
- Val loss: 3.095 ‚Üí 0.721 (77% reduction)
- Test loss: 2.455, Test ppl: 11.648

**Memory:**
- Peak: 7.285 GB (excellent - much better than Phase 1's 18.9 GB!)

### Lessons Learned

1. **Training data format matters critically**
   - Markdown tool calls ‚Üí teaches text generation
   - Structured API format ‚Üí teaches function calling

2. **Human-readable ‚â† Machine-learnable**
   - Examples for docs/demos aren't suitable for training
   - Need examples that match the actual API structure

3. **The `{text}` format works**
   - Phase 1 showed model CAN learn to call tools with this format
   - Just need real results, not placeholders

4. **Memory is NOT the bottleneck**
   - Phase 2 used only 7.3 GB (vs Phase 1's 18.9 GB)
   - Batch size and sequence length matter more than model size

### Next Steps (Phase 2B)

**Fix the generator (lines 212-224 in `generate_training_data.py`):**
1. Bug: `hasattr(part, "tool_name")` matches both ToolCallPart AND ToolReturnPart
2. Fix: Use `part_kind` discriminator to distinguish them
3. Capture real tool results from `ToolReturnPart.content`
4. Regenerate training data with 30B server
5. Convert to `{text}` format (not `{messages}`)
6. Retrain and verify infinite loop is fixed

---

## Future Phases (Planned)

### Phase 3: Scale Up Training Data (TODO)
- **Goal:** Generate 1,000+ examples from 10+ diverse codebases
- **Target:** Achieve autonomous tool use (not memorization)
- **Expected:** Slower than Claude, but works on new codebases

### Phase 3: Optimize Inference Speed (TODO)
- **Optimizations to try:**
  - Fix stop token handling (~15s, 1.4x speedup)
  - Speculative decoding (~8s, 2.6x speedup)
  - KV cache optimization (~7s, 3x speedup)
  - Train for conciseness (~5s, 4.2x speedup)
- **Target:** 6-8 seconds (still slower than Claude's 2.4s, but acceptable)

### Phase 4: Hybrid Architecture (TODO)
- **Goal:** Match Claude Code speed while staying offline
- **Approach:**
  - Intent classifier (<0.1s)
  - Direct tool executor (~0.5s)
  - Response formatter with 7B (~2s)
  - Total: ~3s
- **Target:** <3 seconds, fully autonomous, offline

---

## Metrics to Track Each Phase

For consistency, measure each phase with the same test suite:

### Standard Test Query
"Which classes in this codebase subclass from Protocol?"

### Metrics to Record
1. **Speed:** Time from query to final answer
2. **Tool calls:** Number and type of tools used
3. **Autonomous:** Did model decide to use tools? (Yes/No)
4. **Accuracy:** Found all protocol classes? (%)
5. **Memory:** Peak inference memory (GB)
6. **Model size:** Total disk space (GB)
7. **Training cost:** Time, peak memory, examples used

### Additional Test Queries (For Phase 2+)
1. "Find all async functions in this codebase"
2. "What files import the requests library?"
3. "Show me all dataclasses"
4. "List TODO comments in the source code"

---

## Version History

- **2026-02-13:** Phase 0 baseline established, Phase 1 fixes in progress
- **2026-02-12:** Initial knowledge distillation experiment (7B from 30B)
- **2026-02-12:** 30B vs Claude Code benchmark completed

---

## Related Documentation

- `KNOWLEDGE_DISTILLATION_SUMMARY.md` - Full Phase 0 experiment details
- `BENCHMARK_COMPARISON.md` - Initial 30B vs Claude Code analysis
- `TRAINING_SUMMARY.md` - 1.5B model evaluation results
- `docs/research/training-journal.md` - Detailed training history
