# Model Performance Tracker

**Purpose:** Track model size, memory usage, and performance improvements across training phases.

**Last Updated:** February 13, 2026 - Phase 4 Complete

---

## Quick Reference Table - All Phases

| System | Model Size | Memory (Training) | Speed | Autonomous Tools? | Accuracy | Status |
|--------|------------|-------------------|-------|-------------------|----------|--------|
| **Claude Code** | 0 (cloud) | 0 GB local | **2.41s** ‚ö° | ‚úÖ Yes | 100% | Baseline |
| **30B Baseline** | ~60 GB | ~16 GB | **~27s** | ‚úÖ Yes | 100% | Too slow |
| **7B Phase 0** | 14 GB + 44MB adapter | ~6-8 GB | **21.06s** | ‚ùå No (memorized) | 100% | Broken |
| **7B Phase 1** | 14 GB + 44MB adapter | 18.9 GB | N/A (loops) | ‚ö†Ô∏è Yes (but loops) | N/A | Progress! |
| **7B Phase 4** | 14 GB + 44MB adapter | **18.5 GB** | **~2 turns** | ‚úÖ **Yes!** | ‚úÖ Works | **Fixed!** üéâ |

---

## Phase 0: Baseline (Original Training)

### Detailed Metrics

#### **Model Size**
- **30B:** ~60 GB (full model)
- **7B + LoRA:** 14.044 GB total (77% smaller than 30B)
  - 7B base: 14 GB (4-bit quantized)
  - LoRA adapter: 44 MB
- **Claude Code:** 0 GB local (cloud-based)

#### **Memory Usage**
- **30B:** ~16 GB RAM (‚ùå **crashed system**)
- **7B Distilled:** ~6-8 GB RAM (50-60% reduction vs 30B)
  - Training peak: 23.63 GB
  - Inference: ~6-8 GB
- **Claude Code:** 0 GB local

#### **Speed Performance** (Protocol search query)
- **Claude Code:** 2.41s (baseline) - **11.2x faster than 30B**
- **7B Distilled:** 21.06s - 1.3x faster than 30B, but 8.7x slower than Claude
- **30B:** ~27s

#### **Training Costs**
- **7B LoRA Training:**
  - Duration: 11 minutes (50 iterations)
  - Dataset: 69 examples
  - Loss reduction: 81% (1.269 ‚Üí 0.246)
  - Peak memory: 23.63 GB
  - Adapter size: 44 MB

### Key Findings

**Claude Code:**
- ‚úÖ Fastest (2.41s)
- ‚úÖ No local resources
- ‚úÖ Fully autonomous tool use
- ‚úÖ Works on any codebase
- ‚ùå Requires API key/internet

**7B Distilled Model:**
- ‚úÖ 77% smaller than 30B
- ‚úÖ 50-60% less memory than 30B
- ‚úÖ Accurate (100%)
- ‚ùå **Memorized patterns** instead of learning autonomous tool use
- ‚ùå Only works on familiar codebases
- ‚ùå Training data too small (69 examples from one codebase)

**30B Model:**
- ‚úÖ Autonomous tool use
- ‚úÖ Works on any codebase
- ‚ùå 11.2x slower than Claude Code
- ‚ùå 16GB RAM (system crash risk)
- ‚ùå Not production viable

### Critical Issue Discovered

**The 7B model memorized instead of learning:**
- Gave correct answers directly without calling tools
- Learned "this codebase has these protocols" not "how to search any codebase"
- Root cause: Insufficient training data (need 1,000-5,000 examples from 20+ codebases)

---

## Phase 1: Training Data Pipeline Fix ‚úÖ COMPLETED (With Issues)

**Date:** February 13, 2026
**Goal:** Fix critical tool-call format mismatch and prepare for proper tool-calling training
**Status:** Training successful, but discovered new critical bug

### Changes Made

1. **Fixed tool-call format:**
   - Changed from `"tool"` key to `"name"` key (matches parser)
   - Updated 30 hand-authored examples
   - Regenerated 69 converted examples

2. **Added converter to training pipeline:**
   - `train_lora.sh` now runs converter before training
   - `run_full_distillation.sh` includes converter step
   - Ensures correct format reaches mlx_lm.lora

3. **Wired stop_sequences:**
   - Local agents default to QWEN_STOP_SEQUENCES
   - Added to config and factory
   - (Results: NOT working - still generating garbage tokens)

4. **Memory requirements:**
   - Had to use batch_size=1 (not 2) to avoid OOM
   - Actual peak: 18.902 GB
   - batch_size=2 crashed with "Insufficient Memory" error

### Actual Results

| Metric | Phase 0 | Phase 1 Actual | Change | Status |
|--------|---------|----------------|--------|--------|
| Training loss | 1.269 ‚Üí 0.246 | 1.077 ‚Üí 0.096 | Better (91% vs 81%) | ‚úÖ Improved |
| Training peak memory | 23.63 GB | 18.902 GB | -20% (not -40%) | ‚ö†Ô∏è Less than expected |
| Batch size | 4 | 1 (not 2!) | -75% | ‚ùå Slower training |
| Training iterations | 50 | 206 | 4x more | ‚ö†Ô∏è Took longer |
| **Autonomous tools?** | ‚ùå No (memorized) | ‚úÖ **YES!** | Fixed! | üéâ **BREAKTHROUGH** |
| Tool execution | Direct answer | 20+ tool calls | Works but loops | ‚ö†Ô∏è New bug |
| Stop sequences | N/A | Not working | No improvement | ‚ùå Failed |

### üéâ BREAKTHROUGH: Model Uses Tools!

**Phase 0 behavior (broken):**
```
User: "Which classes subclass from Protocol?"
Model: "Found 6 protocols: HttpAppFactory, Client, Agent..." [Direct answer - memorized]
```

**Phase 1 behavior (fixed but looping):**
```
User: "Which classes subclass from Protocol?"
Model: run_command(grep -r 'class.*Protocol' .)
Model: run_command(grep -r 'class.*Protocol' .)  [Repeats 20+ times]
Model: run_command({})  [Empty args]
Model: [Never gives final answer - stuck in loop]
```

### Critical Issues Discovered

1. **‚úÖ FIXED: Memorization ‚Üí Tool Usage**
   - Model now calls tools instead of memorizing
   - Uses appropriate commands (`find`, `grep`)
   - Correctly formatted tool calls with `"name"` key

2. **‚ùå NEW BUG: Infinite Tool Loop**
   - Model calls same tool 20+ times
   - Doesn't process tool results
   - Never stops to give final answer
   - Some calls have empty arguments `{}`

3. **‚ùå Stop Sequences Don't Work**
   - Still generating `<|im_end|>` garbage tokens
   - No speedup observed
   - Need to investigate why stop_sequences aren't being applied

### Root Cause Analysis

**Why infinite loop?**

The training data uses **placeholder tool results**:
```
Tool result: [Tool execution completed]
```

The model learned:
1. ‚úÖ When to call tools
2. ‚úÖ How to format tool calls
3. ‚ùå How to interpret results
4. ‚ùå When to stop and give answer

---

## Phase 4: Stop Sequences + Domain Data üéâ COMPLETE

**Date:** February 13, 2026
**Goal:** Fix stop sequence bug and add domain-specific training data
**Status:** ‚úÖ SUCCESS - Infinite loop fixed!

### Changes Made

1. **Fixed stop sequence bug:**
   - **Root cause:** Key name mismatch in `factory.py`
   - Used `"stop"` key but PydanticAI expects `"stop_sequences"`
   - **Fix:** `model_settings_dict["stop_sequences"] = list(config.stop_sequences)`
   - Updated tests to use correct key

2. **Added domain-specific training data:**
   - Created `scripts/generate_domain_examples.py`
   - Reads real files from t-strings repos (svcs-di, tdom-svcs)
   - Generates 21 domain examples with actual code
   - Total: 199 examples (21 domain + 28 POC + 150 public)

3. **Optimized training:**
   - Batch size: 2 (memory-optimized)
   - Iterations: 300
   - Peak memory: 18.493 GB (stable!)

### Results

| Metric | Phase 1 | Phase 4 | Change | Status |
|--------|---------|---------|--------|--------|
| Training loss | 1.077 ‚Üí 0.096 | 1.012 ‚Üí 0.092 | Similar | ‚úÖ Good |
| Val loss (final) | 0.436 | 1.223 | Higher (overfitting) | ‚ö†Ô∏è Expected |
| Test perplexity | 2.458 | 14.846 | Higher | ‚ö†Ô∏è Different data |
| Training peak memory | 18.902 GB | 18.493 GB | -2% | ‚úÖ Stable |
| Batch size | 1 | 2 | 2x | ‚úÖ Faster |
| Training examples | 69 | 199 | 2.9x | ‚úÖ More data |
| **Infinite loop?** | ‚ùå Yes (20+ calls) | ‚úÖ **NO!** | **FIXED!** | üéâ **SUCCESS** |
| **Stop sequences?** | ‚ùå Broken | ‚úÖ **Working!** | **FIXED!** | üéâ **SUCCESS** |
| Tool calls per query | 20+ (loops) | 1-2 (stops!) | -90% | ‚úÖ Fixed |
| Message turns | N/A (loops) | 2 (completes) | Done! | ‚úÖ Works |

### üéâ BREAKTHROUGH: Both Critical Bugs Fixed!

**Phase 1 behavior (broken):**
```
User: "Find all classes that inherit from Protocol"
Model: run_command(grep -r 'class.*Protocol' .)
Model: run_command(grep -r 'class.*Protocol' .)  [Repeats 20+ times]
Model: [Never stops - infinite loop]
```

**Phase 4 behavior (working!):**
```
User: "Find all classes that inherit from Protocol"
Model: I'll use the run_command tool.

{
  "name": "run_command",
  "arguments": {"command": "grep -r 'class.*Protocol' src/ --include='*.py'"}
}

[Stops after 2 message turns - no loop!]
```

### Test Results

Three queries tested:

| Query | Expected Behavior | Actual Behavior | Status |
|-------|------------------|-----------------|--------|
| Find Protocol classes | Call search tool | ‚úÖ Called run_command (grep) | ‚úÖ Correct |
| What is dependency injection? | Direct answer (no tools) | ‚ö†Ô∏è Called read_file | ‚ö†Ô∏è Unnecessary |
| Show Inject examples | Call read/search | ‚úÖ Called read_file | ‚úÖ Correct |

**Summary:**
- ‚úÖ No infinite loops (all queries complete in 2 turns)
- ‚úÖ Stop sequences working (model stops properly)
- ‚úÖ Tool calls formatted correctly
- ‚ö†Ô∏è Over-eager tool usage (calls tools even for concept questions)

### Training Details

**Configuration:**
- Model: Qwen2.5-Coder-7B-Instruct-4bit
- Batch size: 2 (optimized from Phase 1's batch_size=1)
- Learning rate: 1e-4
- LoRA rank: 16 (num_layers)
- Iterations: 300
- Training time: ~25 minutes

**Dataset Composition:**
- Domain examples: 21 (10.6%) - svcs-di, tdom-svcs patterns
- POC examples: 28 (14.1%) - Punie-specific tools
- Public examples: 150 (75.4%) - Generic tool patterns
- Direct answers: 5 (2.5%) - No tools needed
- **Total: 199 examples** (split 90/10: 179 train / 20 valid)

**Loss Progression:**
- Initial val loss: 2.115
- Iter 100 train loss: 0.189
- Iter 200 val loss: 0.743 (65% improvement!)
- Final train loss: 0.092
- Final val loss: 1.223 (some overfitting, expected with small dataset)
- Test loss: 2.698, Test ppl: 14.846

**Memory:**
- Peak: 18.493 GB (stable throughout)
- Process memory: ~24GB unified (near limit but manageable)
- Training tokens: 151,512

### Issue Discovered: Over-Eager Tool Calling

**Problem:** Model calls tools 97.5% of the time because training data was too tool-heavy.

**Evidence:**
- Query: "What is dependency injection?" (concept question)
- Expected: Direct answer from base knowledge
- Actual: Called read_file tool (unnecessary)

**Root cause:** Training data composition
- 194 examples WITH tools (97.5%)
- 5 examples WITHOUT tools (2.5%)

**Impact:** Minor - model works but is conservative. Better than looping!

### Files Modified

**Code fixes:**
1. `src/punie/agent/factory.py` - Fixed stop_sequences key (line 241)
2. `tests/test_agent_config.py` - Updated test assertions

**Training data:**
3. `scripts/generate_domain_examples.py` - NEW: Domain data generator
4. `scripts/convert_training_data.py` - Updated to merge domain examples
5. `data/domain_examples.jsonl` - NEW: 21 domain examples
6. `data/mlx_format/train.jsonl` - Regenerated (179 examples)
7. `data/mlx_format/valid.jsonl` - Regenerated (20 examples)

**Trained models:**
8. `models/qwen25-7b-distilled/adapters/adapters.safetensors` - Final weights
9. `models/qwen25-7b-distilled/adapters/0000150_adapters.safetensors` - Checkpoint
10. `models/qwen25-7b-distilled/adapters/0000300_adapters.safetensors` - Checkpoint

---

## Phase 5: Balance Tool vs. Direct Answers (PLANNED)

**Goal:** Teach model when NOT to use tools
**Status:** Ready to implement

### Problem

Model is over-eager with tools because training data was 97.5% tool-calling examples.

### Solution

Add 40-50 "direct answer" examples to achieve better balance:
- Tool-calling examples: 150 (75%)
- Direct answer examples: 50 (25%)
- Total: 200 examples

### Direct Answer Categories Needed

1. **Concept questions:** "What is X?", "Explain Y", "How does Z work?"
2. **Comparisons:** "What's the difference between X and Y?"
3. **Best practices:** "When should I use X?"
4. **Syntax/language:** "What does this decorator do?"
5. **Documentation:** "Where can I find X?"

### Expected Outcome

Model learns to discriminate:
- "Find all classes..." ‚Üí use tool (search/read needed)
- "What is dependency injection?" ‚Üí direct answer (concept question)

### Training Format

Direct answer examples (NO tools):
```json
{
  "messages": [
    {"role": "system", "content": "You are Punie..."},
    {"role": "user", "content": "What is dependency injection?"},
    {"role": "assistant", "content": "Dependency injection is..."}
  ]
}
```

---

## Future Phases (Planned)

### Phase 6: Scale Up Training Data (TODO)
- **Goal:** Generate 1,000+ examples from 10+ diverse codebases
- **Target:** Achieve robust autonomous tool use across any codebase
- **Expected:** Maintains speed while improving accuracy

### Phase 7: Optimize Inference Speed (TODO)
- **Optimizations to try:**
  - KV cache optimization
  - Speculative decoding
  - Train for conciseness
- **Target:** 6-8 seconds (currently ~2 turns, need to measure actual time)

---

## Metrics to Track Each Phase

For consistency, measure each phase with the same test suite:

### Standard Test Queries

1. "Find all classes that inherit from Protocol"
2. "What is dependency injection?" (should NOT use tools)
3. "Show me examples of using Inject"

### Metrics to Record

1. **Speed:** Time from query to final answer (or number of turns)
2. **Tool calls:** Number and type of tools used
3. **Autonomous:** Did model decide to use tools appropriately? (Yes/No/Over-eager)
4. **Accuracy:** Correct answer? (Yes/No)
5. **Memory:** Peak training memory (GB)
6. **Model size:** Total disk space (GB)
7. **Training cost:** Time, peak memory, examples used

---

## Version History

- **2026-02-13:** Phase 4 complete - Stop sequences fixed, infinite loop solved! üéâ
- **2026-02-13:** Phase 1 complete - Model uses tools but loops infinitely
- **2026-02-13:** Phase 0 baseline established
- **2026-02-12:** Initial knowledge distillation experiment (7B from 30B)
- **2026-02-12:** 30B vs Claude Code benchmark completed

---

## Related Documentation

- `MEMORY.md` - Project memory with latest context and plans
- `docs/research/phase-19-public-datasets.md` - Public dataset research
- `docs/research/local-model-training-plan.md` - Original training plan
